# -*- coding: utf-8 -*-
"""AI_System_to_Digitize_Handwritten_Exams_in_German_Language_Studies.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hsGqtytQHeSIASF87Y05P1MpSMWysS9g
"""

!pip install datasets transformers evaluate jiwer

! pip install datasets

!pip install evaluate

import numpy as np
import pandas as pd
from datasets import load_dataset
from PIL import Image
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import Dataset
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
import evaluate
from torch.optim import AdamW
from tqdm.notebook import tqdm

# Load CER metric
cer_metric = evaluate.load("cer")

"""**Dataset** **Description**"""

# Load dataset
dataset = load_dataset('fhswf/german_handwriting')
df = pd.DataFrame(dataset['train'])

df.head()

df.info()

df.isnull().sum()

missing_text_rows = df[df['text'].isnull()]
print("Rows with missing 'text' values:")
print(missing_text_rows)

df = df.dropna(subset=['text'])

df.isnull().sum()

class IAMDataset(Dataset):
    def __init__(self, df, processor, max_target_length=128):
        self.df = df
        self.processor = processor
        self.max_target_length = max_target_length

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        image = self.df['image'][idx]  # PIL.Image object
        text = self.df['text'][idx]

        # Process image
        processed_image = self.processor(image, return_tensors="pt")
        pixel_values = processed_image.pixel_values.squeeze(0)

        # Tokenize text
        labels = self.processor.tokenizer(
            text,
            padding="max_length",
            max_length=self.max_target_length,
            truncation=True,
        ).input_ids
        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]

        return {"pixel_values": pixel_values, "labels": torch.tensor(labels)}

# Split dataset
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
cleaned_train_df = train_df[train_df['text'].notnull() & (train_df['text'] != "")]
cleaned_train_df.reset_index(drop=True, inplace=True)
test_df.reset_index(drop=True, inplace=True)

# Validate and fix images
def validate_images(df):
    for idx, image in enumerate(df['image']):
        try:
            if not isinstance(image, Image.Image):
                raise ValueError(f"Image at index {idx} is not a valid PIL image.")
            if image.mode != "RGB":
                df['image'][idx] = image.convert("RGB")
        except Exception as e:
            print(f"Error at index {idx}: {e}")

validate_images(cleaned_train_df)
validate_images(test_df)

# Instantiate the processor
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten")

# Define datasets
train_dataset = IAMDataset(df=cleaned_train_df, processor=processor)
eval_dataset = IAMDataset(df=test_df, processor=processor)

# Collate function
def collate_fn(batch):
    pixel_values = torch.stack([item["pixel_values"] for item in batch])
    labels = torch.nn.utils.rnn.pad_sequence(
        [item["labels"] for item in batch], batch_first=True, padding_value=-100
    )
    return {"pixel_values": pixel_values, "labels": labels}

# Data loaders
train_dataloader = torch.utils.data.DataLoader(
    train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn
)
eval_dataloader = torch.utils.data.DataLoader(
    eval_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn
)

"""**Model Definition & Fine-tuning**"""

# Initialize model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-stage1")
model.to(device)

# Configure model
model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
model.config.pad_token_id = processor.tokenizer.pad_token_id
model.config.vocab_size = model.config.decoder.vocab_size
model.config.eos_token_id = processor.tokenizer.sep_token_id
model.config.max_length = 64
model.config.early_stopping = True
model.config.no_repeat_ngram_size = 3
model.config.length_penalty = 2.0
model.config.num_beams = 4

# Optimizer
optimizer = AdamW(model.parameters(), lr=5e-5)

"""**Training and Evaluation Code**"""

# Training Loop
for epoch in range(5):
    model.train()  # Set the model to training mode
    train_loss = 0.0  # Initialize training loss accumulator

    for batch in tqdm(train_dataloader, desc=f"Training Epoch {epoch + 1}"):
        # Move batch data to the device
        batch = {k: v.to(device) for k, v in batch.items()}

        # Forward pass
        outputs = model(pixel_values=batch["pixel_values"], labels=batch["labels"])
        loss = outputs.loss

        # Backward pass to calculate gradients
        loss.backward()

        # Step optimizer and zero gradients
        optimizer.step()
        optimizer.zero_grad()

        # Accumulate training loss
        train_loss += loss.item()

    # Print average training loss after each epoch
    print(f"Loss after epoch {epoch}: {train_loss / len(train_dataloader)}")

# Enhanced Evaluation Loop with Correct Batch Skipping
model.eval()  # Set the model to evaluation mode
valid_cer = 0.0  # Initialize CER accumulator
problematic_batches = []  # List to store problematic batches

with torch.no_grad():  # Disable gradient computation during evaluation
    for i, batch in enumerate(tqdm(eval_dataloader, desc="Evaluating")):
        try:
            # Skip specific batches 185 and 186
            if i + 1 in [185, 186]:  # Adjust for 1-based indexing in descriptions
                print(f"Skipping batch {i + 1}")
                problematic_batches.append(batch)  # Save skipped batch for debugging
                continue

            # Move batch data to the device
            batch = {k: v.to(device) for k, v in batch.items()}

            # Debugging: Check batch content
            print(f"\nProcessing batch {i + 1}")
            print(f"Batch keys: {batch.keys()}")
            print(f"Batch pixel_values shape: {batch['pixel_values'].shape}")
            print(f"Batch labels shape: {batch['labels'].shape}")

            # Ensure batch contains valid data
            if batch["pixel_values"].nelement() == 0 or batch["labels"].nelement() == 0:
                raise ValueError(f"Empty data in batch {i + 1}")

            # Generate predictions
            outputs = model.generate(pixel_values=batch["pixel_values"])
            print(f"Generated outputs shape: {outputs.shape}")

            pred_texts = processor.batch_decode(outputs, skip_special_tokens=True)
            print(f"Predicted texts: {pred_texts[:3]}")  # Show first 3 predictions

            # Process labels for comparison
            label_ids = batch["labels"].clone()
            label_ids[label_ids == -100] = processor.tokenizer.pad_token_id
            label_texts = processor.batch_decode(label_ids, skip_special_tokens=True)
            print(f"Label texts: {label_texts[:3]}")  # Show first 3 labels

            # Compute CER
            cer = cer_metric.compute(predictions=pred_texts, references=label_texts)
            print(f"Current CER: {cer}")
            valid_cer += cer

        except Exception as e:
            print(f"Error in batch {i + 1}: {e}")
            problematic_batches.append(batch)  # Save problematic batch
            continue  # Skip the problematic batch and proceed

# Final CER calculation
num_skipped_batches = len(problematic_batches)
if valid_cer > 0:
    avg_cer = valid_cer / (len(eval_dataloader) - num_skipped_batches)  # Exclude skipped batches
    print(f"Validation CER after evaluation: {avg_cer:.4f}")
else:
    print("No valid CER calculated due to errors.")



"""**Testing Code**"""

import torch

# Set the model to evaluation mode
model.eval()

# Function to test a single input line
def test_model_on_line(input_text, processor, model, device):
    try:
        # Preprocess the input text
        inputs = processor(text=input_text, return_tensors="pt", padding=True, truncation=True)
        inputs = {k: v.to(device) for k, v in inputs.items()}  # Move to device

        # Generate predictions
        with torch.no_grad():
            outputs = model.generate(pixel_values=inputs["pixel_values"])

        # Decode the predicted text
        predicted_text = processor.batch_decode(outputs, skip_special_tokens=True)[0]

        print("\n===== Model Prediction =====")
        print(f"Input Text: {input_text}")
        print(f"Predicted Text: {predicted_text}\n")

        return predicted_text
    except Exception as e:
        print(f"Error during testing: {e}")
        return None

# Example usage:
input_text = "Your sample input line here"  # Replace with the actual input
predicted_output = test_model_on_line(input_text, processor, model, device)



"""**Extracting Text From Image**"""

import easyocr
reader = easyocr.Reader(['de'])
translator = Translator()

import PIL
from PIL import ImageDraw
im = PIL.Image.open("/content/Screenshot 2025-01-07 215746.png")

bounds = reader.readtext('/content/Screenshot 2025-01-07 215746.png', add_margin=0.55, width_ths=0.7, link_threshold=0.8, decoder='beamsearch',blocklist='=-')
bounds

def draw_boxes(image, bounds, color='yellow', width=2):
    draw = ImageDraw.Draw(image)
    for bound in bounds:
        p0, p1, p2, p3 = bound[0]
        draw.line([*p0, *p1, *p2, *p3, *p0], fill=color, width=width)
    return image

draw_boxes(im, bounds)

text_list = reader.readtext('/content/Screenshot 2025-01-07 215746.png', add_margin=0.55, width_ths=0.7, link_threshold=0.8, decoder='beamsearch',blocklist='=-', detail=0)
text_list

text_comb=' '.join(text_list)
text_comb











